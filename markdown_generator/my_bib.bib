%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Michael Celentano at 2023-09-20 18:14:28 -0700 


%% Saved with string encoding Unicode (UTF-8) 



@article{celentano2021,
	author = {Celentano, Michael},
	date-added = {2023-09-20 17:40:08 -0700},
	date-modified = {2023-09-20 17:57:47 -0700},
	doi = {10.1093/imaiai/iaaa037},
	issn = {2049-8772},
	journal = {Information and Inference: A Journal of the IMA},
	month = {Jan},
	number = {3},
	pages = {1105-1165},
	title = {Approximate separability of symmetrically penalized least squares in high dimensions: characterization and consequences},
	url = {https://doi.org/10.1093/imaiai/iaaa037},
	volume = {10},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1093/imaiai/iaaa037}}

@inproceedings{CelentanoMontanariWu2020,
	abstract = { Modern large-scale statistical models require the estimation of thousands to millions of parameters. This is often accomplished by iterative algorithms such as gradient descent, projected gradient descent or their accelerated versions. What are the fundamental limits of these approaches? This question is well understood from an optimization viewpoint when the underlying objective is convex. Work in this area characterizes the gap to global optimality as a function of the number of iterations. However, these results have only indirect implications on the gap to \emph{statistical} optimality. Here we consider two families of high-dimensional estimation problems: high-dimensional regression and low-rank matrix estimation, and introduce a class of `general first order methods' that aim at efficiently estimating the underlying parameters. This class of algorithms is broad enough to include classical first order optimization (for convex and non-convex objectives), but also other types of algorithms. Under a random design assumption,  we derive lower bounds on the estimation error that hold in the high-dimensional asymptotics in which both the number of observations and the number of parameters diverge. These lower bounds are optimal in the sense that there exist algorithms in this class whose estimation  error matches the lower bounds up to asymptotically negligible terms. We illustrate our general results through applications to sparse phase retrieval and sparse principal component analysis.},
	author = {Celentano, Michael and Montanari, Andrea and Wu, Yuchen},
	booktitle = {Proceedings of Thirty Third Conference on Learning Theory},
	date-added = {2023-09-20 17:39:02 -0700},
	date-modified = {2023-09-20 18:00:13 -0700},
	editor = {Abernethy, Jacob and Agarwal, Shivani},
	month = {Jul},
	pages = {1078--1141},
	pdf = {http://proceedings.mlr.press/v125/celentano20a/celentano20a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {The estimation error of general first order methods},
	url = {https://proceedings.mlr.press/v125/celentano20a.html},
	volume = {125},
	year = {2020},
	bdsk-url-1 = {https://proceedings.mlr.press/v125/celentano20a.html}}

@article{CelentanoMisiakiewiczMontanari2021,
	archiveprefix = {arXiv},
	author = {Michael Celentano and Theodor Misiakiewicz and Andrea Montanari},
	date-added = {2023-09-20 17:38:14 -0700},
	date-modified = {2023-09-20 17:38:27 -0700},
	eprint = {2103.15996},
	primaryclass = {cs.LG},
	title = {Minimum complexity interpolation in random features models},
	year = {2021}}

@article{CelentanoChengMontanari2021,
	archiveprefix = {arXiv},
	author = {Michael Celentano and Chen Cheng and Andrea Montanari},
	date-added = {2023-09-20 17:37:27 -0700},
	date-modified = {2023-09-20 17:50:46 -0700},
	eprint = {2112.07572},
	journal = {Under Revision at the Annals of Applied Probability},
	primaryclass = {math.PR},
	title = {The high-dimensional asymptotics of first order methods with random data},
	year = {2021}}

@article{Paik2023,
	archiveprefix = {arXiv},
	author = {Seunghoon Paik and Michael Celentano and Alden Green and Ryan J. Tibshirani},
	date-added = {2023-09-20 17:36:37 -0700},
	date-modified = {2023-09-20 18:09:14 -0700},
	eprint = {2309.02422},
	primaryclass = {stat.ML},
	title = {Maximum Mean Discrepancy Meets Neural Networks: The {R}adon-{K}olmogorov-{S}mirnov Test},
	year = {2023}}

@article{CelentanoMontanari2022,
	author = {Michael Celentano and Andrea Montanari},
	date-added = {2023-09-20 17:33:55 -0700},
	date-modified = {2023-09-20 17:58:39 -0700},
	doi = {10.1214/21-AOS2100},
	journal = {The Annals of Statistics},
	keywords = {approximate message passing, computational to statistical gaps, convex, high-dimensional regression, M-estimation, Penalty},
	month = {Feb},
	number = {1},
	pages = {170 -- 196},
	publisher = {Institute of Mathematical Statistics},
	title = {Fundamental barriers to high-dimensional regression with convex penalties},
	url = {https://doi.org/10.1214/21-AOS2100},
	volume = {50},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1214/21-AOS2100}}

@article{Celentano2022,
	archiveprefix = {arXiv},
	author = {Michael Celentano},
	date-added = {2023-09-20 14:43:08 -0700},
	date-modified = {2023-09-20 17:49:36 -0700},
	eprint = {2208.09550},
	journal = {Under revision at the Annals of Probability},
	primaryclass = {math.PR},
	title = {{S}udakov-{F}ernique post-{AMP}, and a new proof of the local convexity of the {TAP} free energy},
	year = {2022}}

@article{CelentanoLinFanMei2023,
	author = {Michael Celentano and Licong Lin and Zhou Fan and Song Mei},
	date-added = {2023-09-20 14:42:35 -0700},
	date-modified = {2023-09-20 18:14:28 -0700},
	journal = {In preparation},
	title = {Mean-field variational inference with the {TAP} free energy: Geometric and statistical properties in linear models},
	year = {2023}}

@article{CelentanoFanMei2023,
	author = {Michael Celentano and Zhou Fan and Song Mei},
	date-added = {2023-09-20 14:39:57 -0700},
	date-modified = {2023-09-20 17:59:42 -0700},
	doi = {10.1214/23-AOS2257},
	journal = {The Annals of Statistics},
	keywords = {approximate message passing, landscape analysis, natural gradient descent, nonconvex optimization, TAP free energy, variational inference, Z2 synchronization},
	month = {Apr},
	number = {2},
	pages = {519 -- 546},
	publisher = {Institute of Mathematical Statistics},
	title = {Local convexity of the {TAP} free energy and {AMP} convergence for {Z2}-synchronization},
	url = {https://doi.org/10.1214/23-AOS2257},
	volume = {51},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1214/23-AOS2257}}

@article{CelentanoWainwright2023,
	archiveprefix = {arXiv},
	author = {Michael Celentano and Martin J. Wainwright},
	date-added = {2023-09-20 13:53:16 -0700},
	date-modified = {2023-09-20 13:53:38 -0700},
	eprint = {2309.01362},
	primaryclass = {math.ST},
	title = {Challenges of the inconsistency regime: Novel debiasing methods for missing data models},
	year = {2023}}

@article{CelentanoMontanari2021,
	archiveprefix = {arXiv},
	author = {Michael Celentano and Andrea Montanari},
	date-added = {2023-09-20 13:36:15 -0700},
	date-modified = {2023-09-20 17:49:44 -0700},
	eprint = {2107.14172},
	journal = {Under revision at Journal of the Royal Statistical Society, Series B},
	primaryclass = {math.ST},
	title = {{CAD}: Debiasing the {L}asso with inaccurate covariate model},
	year = {2021}}

@article{CelentanoMontanariWei2023,
	archiveprefix = {arXiv},
	author = {Michael Celentano and Andrea Montanari and Yuting Wei},
	date-added = {2023-09-20 13:35:16 -0700},
	date-modified = {2023-09-20 18:06:53 -0700},
	eprint = {2007.13716},
	journal = {Accepted at the Annals of Statistics},
	primaryclass = {math.ST},
	title = {The {L}asso with general {G}aussian designs with applications to hypothesis testing},
	year = {2023}}
